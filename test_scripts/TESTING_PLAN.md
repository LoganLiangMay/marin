# Marin Pipeline Testing Plan

**Created:** 2025-11-04
**Status:** Ready for Testing
**Purpose:** Validate complete pipeline before Epic 7 (Production Readiness)

---

## ğŸ“‹ Overview

You have 20 synthetic customer conversation transcripts ready to test the complete Marin processing pipeline. This testing validates:

1. âœ… **Data Loading**: MongoDB integration and call record creation
2. ğŸ¤– **AI Analysis**: AWS Bedrock (Claude) analysis pipeline
3. ğŸ” **Semantic Search**: OpenSearch embedding and RAG
4. ğŸ“Š **Analytics**: Aggregation and insights generation
5. ğŸ“ˆ **Dashboard**: Frontend data display

---

## ğŸ¯ Testing Strategy

### Phase 1: Quick Validation (5 minutes)

**Goal:** Verify basic connectivity and data format

```bash
cd /Applications/Gauntlet/marin/test_scripts

# Set environment variables
export MONGODB_URI="your-mongodb-uri-here"
export REDIS_URL="redis://localhost:6379/0"  # or your Redis endpoint

# Run quick test
python quick_test.py
```

**Expected Results:**
- âœ… MongoDB connection successful
- âœ… Redis connection successful (optional)
- âœ… 20 conversations loaded
- âœ… Sample call created in MongoDB

**If this fails:** Fix connection issues before proceeding.

---

### Phase 2: Data Loading Test (1 minute)

**Goal:** Load all 20 conversations into MongoDB without AI analysis

```bash
# This is safe - no API costs
python test_pipeline.py --env dev --conversations ./ --skip-analysis
```

**Expected Results:**
- âœ… 20 conversations parsed
- âœ… 20 call records created in MongoDB
- âœ… Records spread over 20 days (for time-series testing)
- âœ… Test report generated

**Time:** ~30 seconds
**Cost:** $0

---

### Phase 3: Single Call Analysis (2 minutes)

**Goal:** Test AI analysis on ONE call to validate it works

```bash
# Get a call ID from Phase 2
mongosh $MONGODB_URI --eval "db.calls.findOne({status: 'transcribed'}, {call_id: 1})"

# Manually trigger analysis (in Python)
python << 'EOF'
import sys
sys.path.insert(0, '/Applications/Gauntlet/marin/backend')
from workers.tasks import analyze_call_task

call_id = "YOUR_CALL_ID_HERE"  # Replace with actual ID
result = analyze_call_task(call_id)

if result.get('success'):
    print("âœ… Analysis successful!")
    print(f"Sentiment: {result.get('sentiment')}")
    print(f"Pain points: {len(result.get('pain_points', []))}")
else:
    print(f"âŒ Analysis failed: {result.get('error')}")
EOF
```

**Expected Results:**
- âœ… Call status changed to "analyzed"
- âœ… Sentiment detected (positive/neutral/negative)
- âœ… Pain points extracted (5-10 typically)
- âœ… Entities identified (companies, people, products)
- âœ… Objections captured

**Time:** ~15-20 seconds
**Cost:** ~$0.015-0.025

**If this works:** Proceed to Phase 4
**If this fails:** Check AWS Bedrock access and credentials

---

### Phase 4: Full Pipeline Test (10 minutes)

**Goal:** Process all 20 conversations through complete pipeline

âš ï¸ **Warning:** This makes 20 API calls to AWS Bedrock
**Estimated Cost:** $0.30-0.50

```bash
# Run full pipeline
python test_pipeline.py --env dev --conversations ./

# It will ask for confirmation:
# Continue with analysis? (yes/no): yes
```

**Expected Results:**
- âœ… 20 analyses completed (100% success rate)
- âœ… 20 embeddings created for semantic search
- âœ… Analytics data populated
- âœ… Comprehensive test report generated

**Time:** ~8-10 minutes
**Cost:** ~$0.30-0.50

---

### Phase 5: Analytics Validation (2 minutes)

**Goal:** Verify aggregated data and insights

```bash
# Check sentiment distribution
mongosh $MONGODB_URI << 'EOF'
use audio_pipeline
db.calls.aggregate([
  {$match: {status: 'analyzed'}},
  {$group: {_id: '$analysis.overall_sentiment', count: {$sum: 1}}}
])
EOF

# Check top pain points
mongosh $MONGODB_URI << 'EOF'
use audio_pipeline
db.calls.aggregate([
  {$match: {status: 'analyzed'}},
  {$unwind: '$analysis.pain_points'},
  {$group: {
    _id: '$analysis.pain_points.pain_point',
    count: {$sum: 1},
    avg_severity: {$avg: {
      $switch: {
        branches: [
          {case: {$eq: ['$analysis.pain_points.severity', 'low']}, then: 1},
          {case: {$eq: ['$analysis.pain_points.severity', 'medium']}, then: 2},
          {case: {$eq: ['$analysis.pain_points.severity', 'high']}, then: 3}
        ],
        default: 0
      }
    }}
  }},
  {$sort: {count: -1}},
  {$limit: 10}
])
EOF

# Check entities
mongosh $MONGODB_URI << 'EOF'
use audio_pipeline
db.calls.aggregate([
  {$match: {status: 'analyzed'}},
  {$unwind: '$analysis.entities'},
  {$group: {
    _id: {text: '$analysis.entities.text', type: '$analysis.entities.type'},
    count: {$sum: 1}
  }},
  {$sort: {count: -1}},
  {$limit: 20}
])
EOF
```

**Expected Results:**
- âœ… Sentiment distribution (mix of positive/neutral/negative)
- âœ… Top pain points ranked by frequency
- âœ… Common entities identified (Meta, Google, TikTok, Shopify, etc.)
- âœ… Severity scoring working

---

### Phase 6: Frontend Testing (5 minutes)

**Goal:** Verify dashboard displays data correctly

```bash
# Start frontend (if not already running)
cd /Applications/Gauntlet/marin/frontend
npm run dev

# Open browser to http://localhost:3000
```

**Manual Tests:**
1. **Login** â†’ Should use Cognito authentication
2. **Dashboard** â†’ Should show call volume metrics
3. **Call Library** â†’ Should list 20 calls
4. **Call Detail** â†’ Click any call, should show:
   - âœ… Transcript
   - âœ… Sentiment analysis
   - âœ… Pain points
   - âœ… Entities
   - âœ… Objections (if any)
5. **Analytics** â†’ Should show:
   - âœ… Call volume chart (last 20 days)
   - âœ… Sentiment distribution pie chart
   - âœ… Top pain points
   - âœ… Top entities bar chart
6. **Insights** â†’ Should show daily insights
7. **Quality** â†’ Should show quality metrics

---

## ğŸ“Š Success Criteria

Before proceeding to Epic 7, validate:

- [x] **Data Loading**: 100% success rate (20/20 conversations)
- [ ] **AI Analysis**: â‰¥95% success rate (19/20 minimum)
- [ ] **Embeddings**: â‰¥90% generated (18/20 minimum)
- [ ] **Analytics**: Data aggregates correctly
- [ ] **Frontend**: All pages render without errors
- [ ] **Performance**: Analysis completes in <30s per call
- [ ] **Cost**: Total test cost <$1.00

---

## ğŸš¨ Common Issues & Solutions

### Issue 1: MongoDB Connection Timeout

```
âŒ Connection failed: ServerSelectionTimeoutError
```

**Solutions:**
1. Verify MongoDB URI:
   ```bash
   echo $MONGODB_URI
   ```
2. Check IP whitelist in MongoDB Atlas
3. Test connection:
   ```bash
   mongosh $MONGODB_URI --eval "db.adminCommand('ping')"
   ```

### Issue 2: AWS Bedrock Access Denied

```
âŒ Analysis failed: AccessDeniedException
```

**Solutions:**
1. Check AWS credentials:
   ```bash
   aws sts get-caller-identity
   ```
2. Verify model access in AWS Console:
   - Navigate to: https://console.aws.amazon.com/bedrock/home#/modelaccess
   - Enable: Anthropic Claude 3 Sonnet
   - Enable: Amazon Titan Embeddings G1 - Text
3. Check region is `us-east-1`

### Issue 3: Analysis Taking Too Long

```
â° Analysis taking >60s per call
```

**Possible Causes:**
- API throttling
- Large transcript size
- Network latency

**Solutions:**
1. Add delays between calls
2. Check AWS Bedrock quotas
3. Consider batch processing

### Issue 4: Embeddings Not Created

```
ğŸ“Š Calls with embeddings: 0/20
```

**Solutions:**
1. Check OpenSearch Serverless is deployed
2. Verify IAM permissions for OpenSearch
3. Check worker is configured to generate embeddings
4. Review logs: `tail -f ../backend/logs/app.log`

---

## ğŸ“ˆ Performance Benchmarks

Based on typical runs:

| Metric | Expected | Acceptable | Poor |
|--------|----------|------------|------|
| **Conversation Loading** | <0.1s/file | <0.5s/file | >1s/file |
| **Call Record Creation** | <0.2s/record | <1s/record | >2s/record |
| **AI Analysis** | 15-20s/call | 20-30s/call | >30s/call |
| **Embedding Generation** | 2-3s/call | 3-5s/call | >5s/call |
| **Total Test Time** | 8-10min | 10-15min | >15min |
| **Success Rate** | 100% | â‰¥95% | <95% |

---

## ğŸ’° Cost Tracking

Track costs during testing:

| Operation | Model | Cost/Call | Qty | Total |
|-----------|-------|-----------|-----|-------|
| AI Analysis | Claude 3 Sonnet | $0.015-0.025 | 20 | $0.30-0.50 |
| Embeddings | Titan Embed | $0.0001-0.0002 | 20 | $0.002-0.004 |
| OpenSearch | Serverless | $0.24/OCU-hour | 0.1hr | $0.024 |
| **Total** | | | | **$0.33-0.53** |

**AWS Cost Explorer:** Check actual costs 24 hours after testing

---

## ğŸ“ Test Report Template

After completing all phases, document results:

```markdown
# Marin Pipeline Test Report

**Date:** 2025-11-04
**Tester:** [Your Name]
**Environment:** dev

## Summary
- âœ…/âŒ All phases completed
- Success Rate: X%
- Total Time: X minutes
- Total Cost: $X.XX

## Results by Phase

### Phase 1: Quick Validation
- Status: âœ…/âŒ
- Notes: ...

### Phase 2: Data Loading
- Conversations Loaded: 20/20
- Calls Created: 20/20
- Status: âœ…/âŒ

### Phase 3: Single Analysis
- Call ID: ...
- Status: âœ…/âŒ
- Sentiment: positive/neutral/negative
- Pain Points: X
- Entities: X

### Phase 4: Full Pipeline
- Analyses Completed: X/20
- Success Rate: X%
- Time: X minutes
- Cost: $X.XX

### Phase 5: Analytics
- Sentiment Distribution: ...
- Top Pain Points: ...
- Top Entities: ...

### Phase 6: Frontend
- Dashboard: âœ…/âŒ
- Call Library: âœ…/âŒ
- Analytics: âœ…/âŒ
- Insights: âœ…/âŒ

## Issues Encountered
1. [Issue description]
   - Resolution: ...
   - Time Lost: X min

## Recommendations
- [ ] Ready for Epic 7
- [ ] Need fixes before Epic 7
- [ ] Additional testing needed

## Next Steps
1. ...
2. ...
```

---

## ğŸ¬ Next Steps After Testing

### If All Tests Pass (âœ… 95%+ success):

**You're ready for Epic 7!**

1. **Document findings** in test report
2. **Archive test data** for future reference
3. **Proceed to Story 7.1:** Production Environment Setup

### If Tests Partially Pass (âš ï¸ 80-95% success):

**Fix issues before Epic 7:**

1. **Analyze failures** in test report
2. **Fix critical bugs** (authentication, analysis, embeddings)
3. **Re-test failed components**
4. **Proceed when â‰¥95% pass rate**

### If Tests Mostly Fail (âŒ <80% success):

**Stop and investigate:**

1. **Review architecture** - is something fundamentally broken?
2. **Check dependencies** - MongoDB, Redis, AWS Bedrock all working?
3. **Validate configuration** - environment variables, credentials
4. **Consider Epic 2-5 fixes** before Epic 7

---

## ğŸ¯ Epic 7 Preview

Once testing is successful, Epic 7 will add:

- **Story 7.1**: Multi-AZ deployment, deletion protection
- **Story 7.2**: CloudWatch dashboards, PagerDuty alerts
- **Story 7.3**: Centralized logging, X-Ray tracing
- **Story 7.4**: Blue/green deployments, rollback procedures
- **Story 7.5**: Load testing, performance optimization
- **Story 7.6**: Security audit, WAF, compliance docs

**Goal:** Transform your working MVP into a production-grade system.

---

## ğŸ“ Support

**Issues during testing?**

1. Check logs: `tail -f /Applications/Gauntlet/marin/backend/logs/app.log`
2. Review test report: `cat test_report_*.json | jq .`
3. Check MongoDB: `mongosh $MONGODB_URI`
4. Check Redis: `redis-cli -u $REDIS_URL ping`
5. AWS Bedrock logs: CloudWatch Logs console

**Still stuck?**
- Review error messages in test output
- Check `TESTING_PLAN.md` troubleshooting section
- Create GitHub issue with test report attached

---

**Ready to start? Run Phase 1!**

```bash
cd /Applications/Gauntlet/marin/test_scripts
python quick_test.py
```

Good luck! ğŸš€
